# CRAWLER

A web crawler (also known in other terms like  automatic indexers, bots, web spiders) is an automated program, or script, that methodically scans or “crawls” through web pages to create an index of the data it is set to look for. This process is called Web crawling or spidering.
There are various uses for web crawlers, but essentially a web crawler is used to collect/mine data from the Internet.
